{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Asignatura: Visión por Computador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Alumnos: Yeray Álvarez-Buylla Parra, María Elena Navarro Santana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Trabajo final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm  \n",
    "from torch.cuda.amp import GradScaler, autocast  \n",
    "from torchvision.ops import sigmoid_focal_loss  \n",
    "import math\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios del modelo 1\n",
    "base_dir = 'C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas/Bone-Coco'\n",
    "train_dir = base_dir + '/train'\n",
    "val_dir = base_dir + '/valid'\n",
    "test_dir = base_dir + '/test'\n",
    "\n",
    "# Directorios del modelo 2\n",
    "base_dir2 = 'C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas2/bone2_coco'\n",
    "train_dir2 = base_dir2 + '/train'\n",
    "val_dir2 = base_dir2 + '/valid'\n",
    "test_dir2 = base_dir2 + '/test'\n",
    "\n",
    "# Directorios del modelo 3\n",
    "base_dir3 = 'C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas3/bone3_coco'\n",
    "train_dir3 = base_dir3 + '/train'\n",
    "val_dir3 = base_dir3 + '/valid'\n",
    "test_dir3 = base_dir3 + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento modelo 1: 1411\n",
      "Tamaño del conjunto de validación modelo 1: 403\n",
      "Tamaño del conjunto de test modelo 1: 201\n",
      "Tamaño del conjunto de entrenamiento modelo 2: 831\n",
      "Tamaño del conjunto de validación modelo 2: 114\n",
      "Tamaño del conjunto de test modelo 2: 49\n",
      "Tamaño del conjunto de entrenamiento modelo 3: 1742\n",
      "Tamaño del conjunto de validación modelo 3: 494\n",
      "Tamaño del conjunto de test modelo 3: 264\n"
     ]
    }
   ],
   "source": [
    "def count_images(directory, extensions=(\"jpg\", \"jpeg\", \"png\")):\n",
    "    # Filtrar archivos solo por las extensiones de imágenes\n",
    "    return len([f for f in os.listdir(directory) \n",
    "                if os.path.isfile(os.path.join(directory, f)) and f.lower().endswith(extensions)])\n",
    "\n",
    "# Contar imágenes en cada conjunto\n",
    "train_count = count_images(train_dir)\n",
    "val_count = count_images(val_dir)\n",
    "test_count = count_images(test_dir)\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento modelo 1:\", train_count)\n",
    "print(\"Tamaño del conjunto de validación modelo 1:\", val_count)\n",
    "print(\"Tamaño del conjunto de test modelo 1:\", test_count)\n",
    "\n",
    "# Contar imágenes en cada conjunto\n",
    "train_count2 = count_images(train_dir2)\n",
    "val_count2 = count_images(val_dir2)\n",
    "test_count2 = count_images(test_dir2)\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento modelo 2:\", train_count2)\n",
    "print(\"Tamaño del conjunto de validación modelo 2:\", val_count2)\n",
    "print(\"Tamaño del conjunto de test modelo 2:\", test_count2)\n",
    "\n",
    "# Contar imágenes en cada conjunto\n",
    "train_count3 = count_images(train_dir3)\n",
    "val_count3 = count_images(val_dir3)\n",
    "test_count3 = count_images(test_dir3)\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento modelo 3:\", train_count3)\n",
    "print(\"Tamaño del conjunto de validación modelo 3:\", val_count3)\n",
    "print(\"Tamaño del conjunto de test modelo 3:\", test_count3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas\\Bone-Yolov11\\valid\\labels.cache... 403 images, 2 backgrounds, 0 corrupt: 100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:03<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        403        661      0.659      0.635       0.65      0.267\n",
      "Speed: 0.6ms preprocess, 3.1ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n",
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas\\Bone-Yolov11\\valid\\labels.cache... 403 images, 2 backgrounds, 0 corrupt: 100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:03<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        403        661      0.225      0.139      0.124     0.0348\n",
      "Speed: 0.5ms preprocess, 3.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val3\u001b[0m\n",
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas\\Bone-Yolov11\\valid\\labels.cache... 403 images, 2 backgrounds, 0 corrupt: 100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:02<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        403        661      0.054     0.0454     0.0141    0.00299\n",
      "Speed: 0.6ms preprocess, 3.1ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val4\u001b[0m\n",
      "     model              precision                 recall     mAP50  mAP50-95\n",
      "0  Model 1   [0.6591128625515691]   [0.6347575049062814]  0.649996  0.267395\n",
      "1  Model 2  [0.22503586942857806]  [0.13918305597579425]  0.123874  0.034837\n",
      "2  Model 3  [0.05395901261848612]   [0.0453857791225416]  0.014051  0.002993\n"
     ]
    }
   ],
   "source": [
    "# Define rutas de los modelos y del conjunto de pruebas del dataset 1\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"Model 1\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas/Bone-Yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Model 2\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas2/bone2_yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Model 3\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas3/bone3_yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Ruta al archivo YAML que define el conjunto de pruebas 1\n",
    "common_test_yaml = r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas/Bone-Yolov11/data/miarchivo.yml\"\n",
    "\n",
    "# Evalua cada modelo en el mismo conjunto de pruebas\n",
    "results = []\n",
    "\n",
    "for model_info in models:\n",
    "    model = YOLO(model_info[\"model_path\"])  # Carga el modelo\n",
    "    metrics = model.val(data=common_test_yaml)  # Evaluar en el conjunto de pruebas 1\n",
    "    results.append({\n",
    "        \"model\": model_info[\"name\"],\n",
    "        \"precision\": metrics.box.p,\n",
    "        \"recall\": metrics.box.r,\n",
    "        \"mAP50\": metrics.box.map50,\n",
    "        \"mAP50-95\": metrics.box.map,\n",
    "    })\n",
    "\n",
    "# Muestra y guardar los resultados\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(by=\"mAP50-95\", ascending=False, inplace=True)\n",
    "\n",
    "print(df)  # Mostrar en consola\n",
    "df.to_csv(\"comparacion_modelo_1.csv\", index=False)  # Guarda en un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas2\\bone2_yolov11\\valid\\labels.cache... 114 images, 0 backgrounds, 0 corrupt: 100%|██████████| 114/114 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:01<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        114        120     0.0968       0.15     0.0402      0.015\n",
      "Speed: 1.6ms preprocess, 4.2ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val5\u001b[0m\n",
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas2\\bone2_yolov11\\valid\\labels.cache... 114 images, 0 backgrounds, 0 corrupt: 100%|██████████| 114/114 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:01<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        114        120      0.527      0.352      0.386      0.141\n",
      "Speed: 1.7ms preprocess, 4.6ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val6\u001b[0m\n",
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas2\\bone2_yolov11\\valid\\labels.cache... 114 images, 0 backgrounds, 0 corrupt: 100%|██████████| 114/114 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:01<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        114        120     0.0918     0.0667     0.0276    0.00695\n",
      "Speed: 0.7ms preprocess, 2.7ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val7\u001b[0m\n",
      "     model              precision                 recall     mAP50  mAP50-95\n",
      "1  Model 2   [0.5266365520346729]   [0.3523256265755877]  0.385535  0.140828\n",
      "0  Model 1  [0.09684600658739738]                 [0.15]  0.040196  0.014970\n",
      "2  Model 3  [0.09182698936346073]  [0.06666666666666667]  0.027572  0.006949\n"
     ]
    }
   ],
   "source": [
    "# Define rutas de los modelos y del conjunto de pruebas del dataset 1\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"Model 1\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas/Bone-Yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Model 2\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas2/bone2_yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Model 3\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas3/bone3_yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Ruta al archivo YAML que define el conjunto de pruebas 1\n",
    "common_test_yaml = r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas2/bone2_yolov11/data/miarchivo.yml\"\n",
    "\n",
    "# Evalua cada modelo en el mismo conjunto de pruebas\n",
    "results = []\n",
    "\n",
    "for model_info in models:\n",
    "    model = YOLO(model_info[\"model_path\"])  # Carga el modelo\n",
    "    metrics = model.val(data=common_test_yaml)  # Evaluar en el conjunto de pruebas 1\n",
    "    results.append({\n",
    "        \"model\": model_info[\"name\"],\n",
    "        \"precision\": metrics.box.p,\n",
    "        \"recall\": metrics.box.r,\n",
    "        \"mAP50\": metrics.box.map50,\n",
    "        \"mAP50-95\": metrics.box.map,\n",
    "    })\n",
    "\n",
    "# Muestra y guardar los resultados\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(by=\"mAP50-95\", ascending=False, inplace=True)\n",
    "\n",
    "print(df)  # Mostrar en consola\n",
    "df.to_csv(\"comparacion_modelo_2.csv\", index=False)  # Guarda en un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas3\\bone3_yolov11\\valid\\labels.cache... 494 images, 2 backgrounds, 0 corrupt: 100%|██████████| 494/494 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        494        560     0.0186     0.0196      0.003   0.000621\n",
      "Speed: 0.4ms preprocess, 2.8ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val8\u001b[0m\n",
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas3\\bone3_yolov11\\valid\\labels.cache... 494 images, 2 backgrounds, 0 corrupt: 100%|██████████| 494/494 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        494        560      0.142      0.111     0.0458    0.00996\n",
      "Speed: 0.3ms preprocess, 3.0ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n",
      "Ultralytics 8.3.40  Python-3.9.21 torch-2.6.0.dev20241230+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas3\\bone3_yolov11\\valid\\labels.cache... 494 images, 2 backgrounds, 0 corrupt: 100%|██████████| 494/494 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        494        560      0.901      0.821      0.886      0.459\n",
      "Speed: 0.4ms preprocess, 3.2ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val10\u001b[0m\n",
      "     model               precision                  recall     mAP50  mAP50-95\n",
      "2  Model 3     [0.901140241906021]    [0.8214285714285714]  0.886305  0.459133\n",
      "1  Model 2    [0.1423219414076907]   [0.11071428571428571]  0.045844  0.009958\n",
      "0  Model 1  [0.018614454595773443]  [0.019642857142857142]  0.003003  0.000621\n"
     ]
    }
   ],
   "source": [
    "# Define rutas de los modelos y del conjunto de pruebas del dataset 1\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"Model 1\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas/Bone-Yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Model 2\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas2/bone2_yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Model 3\",\n",
    "        \"model_path\": r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas3/bone3_yolov11/runs/detect/train/weights/best.pt\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Ruta al archivo YAML que define el conjunto de pruebas 1\n",
    "common_test_yaml = r\"C:/Users/lenin/Documents/Universidad_2024-2025/VC/Trabajo_final/Pruebas3/bone3_yolov11/data/miarchivo.yml\"\n",
    "\n",
    "# Evalua cada modelo en el mismo conjunto de pruebas\n",
    "results = []\n",
    "\n",
    "for model_info in models:\n",
    "    model = YOLO(model_info[\"model_path\"])  # Carga el modelo\n",
    "    metrics = model.val(data=common_test_yaml)  # Evaluar en el conjunto de pruebas 1\n",
    "    results.append({\n",
    "        \"model\": model_info[\"name\"],\n",
    "        \"precision\": metrics.box.p,\n",
    "        \"recall\": metrics.box.r,\n",
    "        \"mAP50\": metrics.box.map50,\n",
    "        \"mAP50-95\": metrics.box.map,\n",
    "    })\n",
    "\n",
    "# Muestra y guardar los resultados\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(by=\"mAP50-95\", ascending=False, inplace=True)\n",
    "\n",
    "print(df)  # Mostrar en consola\n",
    "df.to_csv(\"comparacion_modelo_3.csv\", index=False)  # Guarda en un archivo CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor modelo para su conjunto de datos es el modelo 3 debido a sus métricas batantes altas respecto a otros modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenin\\AppData\\Local\\Temp\\ipykernel_19560\\2002529230.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/10:   0%|          | 0/177 [00:00<?, ?batch/s]C:\\Users\\lenin\\AppData\\Local\\Temp\\ipykernel_19560\\2002529230.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|██████████| 177/177 [01:08<00:00,  2.58batch/s, loss=1.0213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 177/177 [01:10<00:00,  2.52batch/s, loss=1.0706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.0937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 177/177 [01:05<00:00,  2.70batch/s, loss=1.0709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.0669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 177/177 [01:13<00:00,  2.41batch/s, loss=1.1781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 177/177 [01:05<00:00,  2.72batch/s, loss=1.0821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 1.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 177/177 [01:06<00:00,  2.65batch/s, loss=1.2150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.0577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 177/177 [01:12<00:00,  2.43batch/s, loss=0.9700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 177/177 [01:12<00:00,  2.44batch/s, loss=1.0577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 177/177 [01:11<00:00,  2.47batch/s, loss=0.9694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.0573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 177/177 [01:14<00:00,  2.39batch/s, loss=1.0974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.0602\n",
      "Modelo guardado en retinanet_fracture.pth\n"
     ]
    }
   ],
   "source": [
    "# 1. Crea el Dataset personalizado\n",
    "class FractureDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_file, transforms=None):\n",
    "        self.images_path = images_path\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images_info = {img['id']: img for img in data['images']}\n",
    "        self.annotations = self.process_annotations(data['annotations'])\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def process_annotations(self, annotations):\n",
    "        processed = {}\n",
    "        for ann in annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in processed:\n",
    "                processed[img_id] = {'boxes': [], 'labels': []}\n",
    "            bbox = ann['bbox']\n",
    "            x_min, y_min, width, height = bbox\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            processed[img_id]['boxes'].append([x_min, y_min, x_max, y_max])\n",
    "            processed[img_id]['labels'].append(1)  # Clase \"fracture\" siempre es 1\n",
    "        return processed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = list(self.images_info.keys())[idx]\n",
    "        img_info = self.images_info[img_id]\n",
    "        img_path = os.path.join(self.images_path, img_info['file_name'])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Obtiene las anotaciones\n",
    "        if img_id in self.annotations:\n",
    "            annotations = self.annotations[img_id]\n",
    "            boxes = torch.tensor(annotations['boxes'], dtype=torch.float32)\n",
    "            labels = torch.tensor(annotations['labels'], dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), dtype=torch.float32)\n",
    "            labels = torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        # Aplicar transformaciones\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# 2. Define transformaciones\n",
    "class ResizeTransform:\n",
    "    def __call__(self, image):\n",
    "        image = cv2.resize(image, (640, 640)) \n",
    "        return F.to_tensor(image)\n",
    "\n",
    "transforms = ResizeTransform()\n",
    "\n",
    "# 3. Inicializa el dataset y dataloaders\n",
    "images_path = r\"C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas\\Bone-Coco\\train\"\n",
    "annotations_file = os.path.join(images_path, '_annotations.json')\n",
    "\n",
    "dataset = FractureDataset(images_path, annotations_file, transforms=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# 4. Carga RetinaNet preentrenado\n",
    "model = retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "out_channels = model.head.classification_head.conv[0].out_channels \n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "cls_logits = torch.nn.Conv2d(out_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n",
    "torch.nn.init.normal_(cls_logits.weight, std=0.01)\n",
    "torch.nn.init.constant_(cls_logits.bias, -math.log((1 - 0.01) / 0.01))\n",
    "\n",
    "model.head.classification_head.cls_logits = cls_logits\n",
    "\n",
    "# Congela todas las capas excepto la última\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Activa gradiente solo para la última capa (cls_logits)\n",
    "for param in model.head.classification_head.cls_logits.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 5. Configura el optimizador\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=0.0001)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 6. Define la función de entrenamiento\n",
    "def train_one_epoch(model, optimizer, dataloader, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for images, targets in pbar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            try:\n",
    "                with autocast():\n",
    "                    loss_dict = model(images, targets)\n",
    "                    if not loss_dict:\n",
    "                        print(\"Lote vacío, saltando...\")\n",
    "                        continue\n",
    "\n",
    "                    classification_logits = loss_dict.get('classification', torch.tensor(0.0, device=device))\n",
    "                    bbox_regression = loss_dict.get('bbox_regression', torch.tensor(0.0, device=device))\n",
    "\n",
    "                    # Verificar tensores vacíos\n",
    "                    if classification_logits.numel() == 0 or bbox_regression.numel() == 0:\n",
    "                        print(\"Tensor vacío en pérdidas, saltando lote...\")\n",
    "                        continue\n",
    "\n",
    "                    # Cálculo de la pérdida con sigmoide focal\n",
    "                    classification_loss = sigmoid_focal_loss(\n",
    "                        classification_logits,\n",
    "                        torch.zeros_like(classification_logits),\n",
    "                        reduction=\"sum\"\n",
    "                    )\n",
    "\n",
    "                    bbox_loss = bbox_regression.sum()\n",
    "\n",
    "                    losses = classification_loss + bbox_loss\n",
    "\n",
    "                    # Verificar valores numéricamente inestables\n",
    "                    if not torch.isfinite(losses).all():\n",
    "                        print(\"Pérdidas NaN o Inf, restableciendo gradientes y saltando lote...\")\n",
    "                        optimizer.zero_grad()\n",
    "                        scaler.update()\n",
    "                        continue\n",
    "\n",
    "                # Backpropagación y optimización\n",
    "                scaler.scale(losses).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                epoch_loss += losses.item()\n",
    "                pbar.set_postfix({\"loss\": f\"{losses.item():.4f}\"})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en el lote: {e}\")\n",
    "                continue\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 7. Entrena el modelo\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Cambiar a modo de evaluación para pruebas\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Modo de evaluación\n",
    "    results = []\n",
    "    with torch.no_grad():  # Desactiva gradientes\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            results.extend(outputs)\n",
    "    return results\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_one_epoch(model, optimizer, dataloader, device, epoch, num_epochs)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    test_dataloader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    evaluation_results = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "# 8. Guarda el modelo\n",
    "torch.save(model.state_dict(), \"retinanet_fracture.pth\")\n",
    "print(\"Modelo guardado en retinanet_fracture.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenin\\anaconda3\\envs\\Trabajo_Final\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'RetinaNet', 'precision': tensor(0.1091), 'recall': tensor(0.2584), 'mAP50': tensor(0.1091), 'mAP50-95': tensor(0.0265)}]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Carga el modelo entrenado\n",
    "model_loaded = retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Asegura que la capa cls_logits tiene las mismas dimensiones antes de cargar\n",
    "model_loaded.head.classification_head.cls_logits = torch.nn.Conv2d(\n",
    "    out_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(\"retinanet_fracture.pth\", map_location=torch.device('cpu')))\n",
    "model_loaded.eval()\n",
    "\n",
    "\n",
    "# Prepara el conjunto de validación\n",
    "images_path = r\"C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas\\Bone-Coco\\valid\"\n",
    "annotations_file = os.path.join(images_path, '_annotations.json')\n",
    "\n",
    "dataset_val = FractureDataset(images_path, annotations_file, transforms=transforms)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Inicializa la métrica para calcular el mAP\n",
    "metric = MeanAveragePrecision()\n",
    "\n",
    "# Realiza la predicción y calcula las métricas\n",
    "with torch.no_grad():  \n",
    "    for images, targets in dataloader_val:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Realiza las predicciones\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Actualiza la métrica con las predicciones y los valores reales\n",
    "        metric.update(predictions, targets)\n",
    "\n",
    "# Calcula las métricas\n",
    "metrics = metric.compute()\n",
    "\n",
    "# Almacena los resultados\n",
    "results = []\n",
    "results.append({\n",
    "    \"model\": \"RetinaNet\",\n",
    "    \"precision\": metrics[\"map_50\"],  # mAP en IoU=0.5\n",
    "    \"recall\": metrics[\"mar_100\"],    # Recall con 100 detecciones\n",
    "    \"mAP50\": metrics[\"map_50\"],      # mAP en IoU=0.5\n",
    "    \"mAP50-95\": metrics[\"map\"]       # mAP en IoU=0.5:0.95\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenin\\AppData\\Local\\Temp\\ipykernel_19560\\3862803722.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/10:   0%|          | 0/104 [00:00<?, ?batch/s]C:\\Users\\lenin\\AppData\\Local\\Temp\\ipykernel_19560\\3862803722.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|██████████| 104/104 [00:46<00:00,  2.24batch/s, loss=1.0943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 104/104 [00:43<00:00,  2.40batch/s, loss=1.0811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 104/104 [00:43<00:00,  2.37batch/s, loss=1.1351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 104/104 [00:42<00:00,  2.43batch/s, loss=1.0893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1.1269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 104/104 [00:38<00:00,  2.73batch/s, loss=1.1568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 1.1357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 104/104 [00:37<00:00,  2.76batch/s, loss=1.0586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 104/104 [00:37<00:00,  2.75batch/s, loss=1.0849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 104/104 [00:39<00:00,  2.62batch/s, loss=1.0799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1.1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 104/104 [00:38<00:00,  2.72batch/s, loss=1.0704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 104/104 [00:38<00:00,  2.67batch/s, loss=1.1398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.1106\n",
      "Modelo guardado en retinanet_fracture2.pth\n"
     ]
    }
   ],
   "source": [
    "# 1. Crea el Dataset personalizado\n",
    "class FractureDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_file, transforms=None):\n",
    "        self.images_path = images_path\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images_info = {img['id']: img for img in data['images']}\n",
    "        self.annotations = self.process_annotations(data['annotations'])\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def process_annotations(self, annotations):\n",
    "        processed = {}\n",
    "        for ann in annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in processed:\n",
    "                processed[img_id] = {'boxes': [], 'labels': []}\n",
    "            bbox = ann['bbox']\n",
    "            x_min, y_min, width, height = bbox\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            processed[img_id]['boxes'].append([x_min, y_min, x_max, y_max])\n",
    "            processed[img_id]['labels'].append(1)  # Clase \"fracture\" siempre es 1\n",
    "        return processed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = list(self.images_info.keys())[idx]\n",
    "        img_info = self.images_info[img_id]\n",
    "        img_path = os.path.join(self.images_path, img_info['file_name'])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Obtiene las anotaciones\n",
    "        if img_id in self.annotations:\n",
    "            annotations = self.annotations[img_id]\n",
    "            boxes = torch.tensor(annotations['boxes'], dtype=torch.float32)\n",
    "            labels = torch.tensor(annotations['labels'], dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), dtype=torch.float32)\n",
    "            labels = torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        # Aplicar transformaciones\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# 2. Define transformaciones\n",
    "class ResizeTransform:\n",
    "    def __call__(self, image):\n",
    "        image = cv2.resize(image, (640, 640)) \n",
    "        return F.to_tensor(image)\n",
    "\n",
    "transforms = ResizeTransform()\n",
    "\n",
    "# 3. Inicializa el dataset y dataloaders\n",
    "images_path = r\"C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas2\\bone2_coco\\train\"\n",
    "annotations_file = os.path.join(images_path, '_annotations.json')\n",
    "\n",
    "dataset = FractureDataset(images_path, annotations_file, transforms=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# 4. Carga RetinaNet preentrenado\n",
    "model = retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "out_channels = model.head.classification_head.conv[0].out_channels \n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "cls_logits = torch.nn.Conv2d(out_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n",
    "torch.nn.init.normal_(cls_logits.weight, std=0.01)\n",
    "torch.nn.init.constant_(cls_logits.bias, -math.log((1 - 0.01) / 0.01))\n",
    "\n",
    "model.head.classification_head.cls_logits = cls_logits\n",
    "\n",
    "# Congela todas las capas excepto la última\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Activa gradiente solo para la última capa (cls_logits)\n",
    "for param in model.head.classification_head.cls_logits.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 5. Configura el optimizador\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=0.0001)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 6. Define la función de entrenamiento\n",
    "def train_one_epoch(model, optimizer, dataloader, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for images, targets in pbar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            try:\n",
    "                with autocast():\n",
    "                    loss_dict = model(images, targets)\n",
    "                    if not loss_dict:\n",
    "                        print(\"Lote vacío, saltando...\")\n",
    "                        continue\n",
    "\n",
    "                    classification_logits = loss_dict.get('classification', torch.tensor(0.0, device=device))\n",
    "                    bbox_regression = loss_dict.get('bbox_regression', torch.tensor(0.0, device=device))\n",
    "\n",
    "                    # Verificar tensores vacíos\n",
    "                    if classification_logits.numel() == 0 or bbox_regression.numel() == 0:\n",
    "                        print(\"Tensor vacío en pérdidas, saltando lote...\")\n",
    "                        continue\n",
    "\n",
    "                    # Cálculo de la pérdida con sigmoide focal\n",
    "                    classification_loss = sigmoid_focal_loss(\n",
    "                        classification_logits,\n",
    "                        torch.zeros_like(classification_logits),\n",
    "                        reduction=\"sum\"\n",
    "                    )\n",
    "\n",
    "                    bbox_loss = bbox_regression.sum()\n",
    "\n",
    "                    losses = classification_loss + bbox_loss\n",
    "\n",
    "                    # Verificar valores numéricamente inestables\n",
    "                    if not torch.isfinite(losses).all():\n",
    "                        print(\"Pérdidas NaN o Inf, restableciendo gradientes y saltando lote...\")\n",
    "                        optimizer.zero_grad()\n",
    "                        scaler.update()\n",
    "                        continue\n",
    "\n",
    "                # Backpropagación y optimización\n",
    "                scaler.scale(losses).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                epoch_loss += losses.item()\n",
    "                pbar.set_postfix({\"loss\": f\"{losses.item():.4f}\"})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en el lote: {e}\")\n",
    "                continue\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 7. Entrena el modelo\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Cambiar a modo de evaluación para pruebas\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Modo de evaluación\n",
    "    results = []\n",
    "    with torch.no_grad():  # Desactiva gradientes\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            results.extend(outputs)\n",
    "    return results\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_one_epoch(model, optimizer, dataloader, device, epoch, num_epochs)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    test_dataloader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    evaluation_results = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "# 8. Guarda el modelo\n",
    "torch.save(model.state_dict(), \"retinanet_fracture2.pth\")\n",
    "print(\"Modelo guardado en retinanet_fracture2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'RetinaNet', 'precision': tensor(0.0125), 'recall': tensor(0.1733), 'mAP50': tensor(0.0125), 'mAP50-95': tensor(0.0035)}]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Carga el modelo entrenado\n",
    "model_loaded = retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Asegura que la capa cls_logits tiene las mismas dimensiones antes de cargar\n",
    "model_loaded.head.classification_head.cls_logits = torch.nn.Conv2d(\n",
    "    out_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(\"retinanet_fracture2.pth\", map_location=torch.device('cpu')))\n",
    "model_loaded.eval()\n",
    "\n",
    "\n",
    "# Prepara el conjunto de validación\n",
    "images_path = r\"C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas2\\bone2_coco\\valid\"\n",
    "annotations_file = os.path.join(images_path, '_annotations.json')\n",
    "\n",
    "dataset_val = FractureDataset(images_path, annotations_file, transforms=transforms)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Inicializa la métrica para calcular el mAP\n",
    "metric = MeanAveragePrecision()\n",
    "\n",
    "# Realiza la predicción y calcula las métricas\n",
    "with torch.no_grad():  \n",
    "    for images, targets in dataloader_val:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Realiza las predicciones\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Actualiza la métrica con las predicciones y los valores reales\n",
    "        metric.update(predictions, targets)\n",
    "\n",
    "# Calcula las métricas\n",
    "metrics = metric.compute()\n",
    "\n",
    "# Almacena los resultados\n",
    "results = []\n",
    "results.append({\n",
    "    \"model\": \"RetinaNet\",\n",
    "    \"precision\": metrics[\"map_50\"],  # mAP en IoU=0.5\n",
    "    \"recall\": metrics[\"mar_100\"],    # Recall con 100 detecciones\n",
    "    \"mAP50\": metrics[\"map_50\"],      # mAP en IoU=0.5\n",
    "    \"mAP50-95\": metrics[\"map\"]       # mAP en IoU=0.5:0.95\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenin\\AppData\\Local\\Temp\\ipykernel_19560\\201642441.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/10:   0%|          | 0/218 [00:00<?, ?batch/s]C:\\Users\\lenin\\AppData\\Local\\Temp\\ipykernel_19560\\201642441.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|██████████| 218/218 [01:37<00:00,  2.25batch/s, loss=1.0943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 218/218 [01:19<00:00,  2.73batch/s, loss=0.9885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 218/218 [01:18<00:00,  2.77batch/s, loss=0.9720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.0261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 218/218 [01:19<00:00,  2.75batch/s, loss=1.0530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 218/218 [01:19<00:00,  2.75batch/s, loss=1.0066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 1.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 218/218 [01:19<00:00,  2.73batch/s, loss=0.8961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.0101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 218/218 [01:28<00:00,  2.46batch/s, loss=0.9336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 218/218 [01:19<00:00,  2.73batch/s, loss=0.9386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 218/218 [01:21<00:00,  2.68batch/s, loss=0.9863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 218/218 [01:18<00:00,  2.79batch/s, loss=0.9748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.0053\n",
      "Modelo guardado en retinanet_fracture3.pth\n"
     ]
    }
   ],
   "source": [
    "# 1. Crea el Dataset personalizado\n",
    "class FractureDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_file, transforms=None):\n",
    "        self.images_path = images_path\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images_info = {img['id']: img for img in data['images']}\n",
    "        self.annotations = self.process_annotations(data['annotations'])\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def process_annotations(self, annotations):\n",
    "        processed = {}\n",
    "        for ann in annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in processed:\n",
    "                processed[img_id] = {'boxes': [], 'labels': []}\n",
    "            bbox = ann['bbox']\n",
    "            x_min, y_min, width, height = bbox\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            processed[img_id]['boxes'].append([x_min, y_min, x_max, y_max])\n",
    "            processed[img_id]['labels'].append(1)  # Clase \"fracture\" siempre es 1\n",
    "        return processed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = list(self.images_info.keys())[idx]\n",
    "        img_info = self.images_info[img_id]\n",
    "        img_path = os.path.join(self.images_path, img_info['file_name'])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Obtiene las anotaciones\n",
    "        if img_id in self.annotations:\n",
    "            annotations = self.annotations[img_id]\n",
    "            boxes = torch.tensor(annotations['boxes'], dtype=torch.float32)\n",
    "            labels = torch.tensor(annotations['labels'], dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), dtype=torch.float32)\n",
    "            labels = torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        # Aplicar transformaciones\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# 2. Define transformaciones\n",
    "class ResizeTransform:\n",
    "    def __call__(self, image):\n",
    "        image = cv2.resize(image, (640, 640)) \n",
    "        return F.to_tensor(image)\n",
    "\n",
    "transforms = ResizeTransform()\n",
    "\n",
    "# 3. Inicializa el dataset y dataloaders\n",
    "images_path = r\"C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas3\\bone3_coco\\train\"\n",
    "annotations_file = os.path.join(images_path, '_annotations.json')\n",
    "\n",
    "dataset = FractureDataset(images_path, annotations_file, transforms=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# 4. Carga RetinaNet preentrenado\n",
    "model = retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "out_channels = model.head.classification_head.conv[0].out_channels \n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "cls_logits = torch.nn.Conv2d(out_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n",
    "torch.nn.init.normal_(cls_logits.weight, std=0.01)\n",
    "torch.nn.init.constant_(cls_logits.bias, -math.log((1 - 0.01) / 0.01))\n",
    "\n",
    "model.head.classification_head.cls_logits = cls_logits\n",
    "\n",
    "# Congela todas las capas excepto la última\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Activa gradiente solo para la última capa (cls_logits)\n",
    "for param in model.head.classification_head.cls_logits.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 5. Configura el optimizador\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=0.0001)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 6. Define la función de entrenamiento\n",
    "def train_one_epoch(model, optimizer, dataloader, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for images, targets in pbar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            try:\n",
    "                with autocast():\n",
    "                    loss_dict = model(images, targets)\n",
    "                    if not loss_dict:\n",
    "                        print(\"Lote vacío, saltando...\")\n",
    "                        continue\n",
    "\n",
    "                    classification_logits = loss_dict.get('classification', torch.tensor(0.0, device=device))\n",
    "                    bbox_regression = loss_dict.get('bbox_regression', torch.tensor(0.0, device=device))\n",
    "\n",
    "                    # Verificar tensores vacíos\n",
    "                    if classification_logits.numel() == 0 or bbox_regression.numel() == 0:\n",
    "                        print(\"Tensor vacío en pérdidas, saltando lote...\")\n",
    "                        continue\n",
    "\n",
    "                    # Cálculo de la pérdida con sigmoide focal\n",
    "                    classification_loss = sigmoid_focal_loss(\n",
    "                        classification_logits,\n",
    "                        torch.zeros_like(classification_logits),\n",
    "                        reduction=\"sum\"\n",
    "                    )\n",
    "\n",
    "                    bbox_loss = bbox_regression.sum()\n",
    "\n",
    "                    losses = classification_loss + bbox_loss\n",
    "\n",
    "                    # Verificar valores numéricamente inestables\n",
    "                    if not torch.isfinite(losses).all():\n",
    "                        print(\"Pérdidas NaN o Inf, restableciendo gradientes y saltando lote...\")\n",
    "                        optimizer.zero_grad()\n",
    "                        scaler.update()\n",
    "                        continue\n",
    "\n",
    "                # Backpropagación y optimización\n",
    "                scaler.scale(losses).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                epoch_loss += losses.item()\n",
    "                pbar.set_postfix({\"loss\": f\"{losses.item():.4f}\"})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en el lote: {e}\")\n",
    "                continue\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 7. Entrena el modelo\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Cambiar a modo de evaluación para pruebas\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Modo de evaluación\n",
    "    results = []\n",
    "    with torch.no_grad():  # Desactiva gradientes\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            results.extend(outputs)\n",
    "    return results\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_one_epoch(model, optimizer, dataloader, device, epoch, num_epochs)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    test_dataloader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    evaluation_results = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "# 8. Guarda el modelo\n",
    "torch.save(model.state_dict(), \"retinanet_fracture3.pth\")\n",
    "print(\"Modelo guardado en retinanet_fracture3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'RetinaNet', 'precision': tensor(0.1355), 'recall': tensor(0.3471), 'mAP50': tensor(0.1355), 'mAP50-95': tensor(0.0367)}]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Carga el modelo entrenado\n",
    "model_loaded = retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Asegura que la capa cls_logits tiene las mismas dimensiones antes de cargar\n",
    "model_loaded.head.classification_head.cls_logits = torch.nn.Conv2d(\n",
    "    out_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(\"retinanet_fracture3.pth\", map_location=torch.device('cpu')))\n",
    "model_loaded.eval()\n",
    "\n",
    "\n",
    "# Prepara el conjunto de validación\n",
    "images_path = r\"C:\\Users\\lenin\\Documents\\Universidad_2024-2025\\VC\\Trabajo_final\\Pruebas3\\bone3_coco\\valid\"\n",
    "annotations_file = os.path.join(images_path, '_annotations.json')\n",
    "\n",
    "dataset_val = FractureDataset(images_path, annotations_file, transforms=transforms)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Inicializa la métrica para calcular el mAP\n",
    "metric = MeanAveragePrecision()\n",
    "\n",
    "# Realiza la predicción y calcula las métricas\n",
    "with torch.no_grad():  \n",
    "    for images, targets in dataloader_val:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Realiza las predicciones\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Actualiza la métrica con las predicciones y los valores reales\n",
    "        metric.update(predictions, targets)\n",
    "\n",
    "# Calcula las métricas\n",
    "metrics = metric.compute()\n",
    "\n",
    "# Almacena los resultados\n",
    "results = []\n",
    "results.append({\n",
    "    \"model\": \"RetinaNet\",\n",
    "    \"precision\": metrics[\"map_50\"],  # mAP en IoU=0.5\n",
    "    \"recall\": metrics[\"mar_100\"],    # Recall con 100 detecciones\n",
    "    \"mAP50\": metrics[\"map_50\"],      # mAP en IoU=0.5\n",
    "    \"mAP50-95\": metrics[\"map\"]       # mAP en IoU=0.5:0.95\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trabajo_Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
