{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Asignatura: Visión por Computador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Alumnos: Yeray Álvarez-Buylla Parra, María Elena Navarro Santana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Práctica 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Máscara en mediapipe\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        frame = cv2.flip(frame,1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        if results.multi_face_landmarks is not None:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, face_landmarks,\n",
    "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=1, circle_radius=1),\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=1))\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen de cabello cargada correctamente. Dimensiones: (505, 494, 4)\n"
     ]
    }
   ],
   "source": [
    "# Carga la textura para los labios\n",
    "lip_texture = cv2.imread('C:/Users/lenin/Pictures/labios_negro.jpg')\n",
    "if lip_texture is None:\n",
    "    print(\"Error: No se pudo cargar la textura de los labios. Verifica la ruta del archivo.\")\n",
    "    exit()\n",
    "lip_red_texture = cv2.imread('C:/Users/lenin/Pictures/pintalabios_textura.jpg')\n",
    "if lip_texture is None:\n",
    "    print(\"Error: No se pudo cargar la textura de los labios. Verifica la ruta del archivo.\")\n",
    "    exit()\n",
    "# Carga la imagen de cabello usando PIL para manejar el canal alfa\n",
    "try:\n",
    "    hair_image_pil = Image.open('C:/Users/lenin/Documents/Universidad_2024-2025/VC/Práctica_5/P5/Filtros/straight_hair.png')\n",
    "    hair_image = np.array(hair_image_pil)\n",
    "    hair_image_pil_red = Image.open('C:/Users/lenin/Documents/Universidad_2024-2025/VC/Práctica_5/P5/Filtros/red_hair.png')\n",
    "    \n",
    "    hair_image_red = np.array(hair_image_pil_red)\n",
    "    print(\"Imagen de cabello cargada correctamente. Dimensiones:\", hair_image.shape)\n",
    "except Exception as e:\n",
    "    print(\"Error: No se pudo cargar la imagen de cabello con Pillow. Detalles:\", e)\n",
    "    exit()\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# Índices de los puntos exteriores de los labios y límites de la cara\n",
    "OUTER_LIPS = [78,191,80,81,82,13,312,311,310,415,308,324,318,402,317,14,87,178,88,95,185,40,39,37,0,267,269,270,409,291,375,321,405,314,17,84,181,91,146]\n",
    "FOREHEAD = 10\n",
    "CHIN = 152\n",
    "LEFT_FACE = 234\n",
    "RIGHT_FACE = 454\n",
    "\n",
    "scale_factor = 3.0\n",
    "filter_active = False  # Para activar o desactivar el filtro\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "       \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_rgb = frame\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            frame_gray = cv2.cvtColor(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            if filter_active:\n",
    "                    frame_filter = frame_gray\n",
    "                    lip=lip_texture\n",
    "                    hair=hair_image_pil\n",
    "                    shape_factor=(2.25,3.25)\n",
    "                    position_offset=(0.2,2)\n",
    "            else:\n",
    "                    frame_filter = frame\n",
    "                    lip=lip_red_texture\n",
    "                    hair=hair_image_pil_red\n",
    "                    shape_factor=(2.4,3)\n",
    "                    position_offset=(0.3,2)\n",
    "\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # *** Superponer labios con textura negra ***\n",
    "                outer_lips_points = [(int(face_landmarks.landmark[i].x * frame.shape[1]),\n",
    "                                      int(face_landmarks.landmark[i].y * frame.shape[0])) for i in OUTER_LIPS]\n",
    "                \n",
    "                mask = np.zeros_like(frame, dtype=np.uint8)\n",
    "                outer_lips_points_array = np.array([outer_lips_points], np.int32)\n",
    "                cv2.fillPoly(mask, [outer_lips_points_array], (255, 255, 255))\n",
    "                \n",
    "                \n",
    "                resized_texture = cv2.resize(lip, (frame_filter.shape[1], frame_filter.shape[0]))\n",
    "                \n",
    "                lip_area = cv2.bitwise_and(resized_texture, resized_texture, mask=cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY))\n",
    "                background = cv2.bitwise_and(frame_filter, frame_filter, mask=cv2.bitwise_not(cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)))\n",
    "                frame = cv2.add(background, lip_area)\n",
    "                # *** Superponer cabello ***\n",
    "                forehead_point = face_landmarks.landmark[FOREHEAD]\n",
    "                chin_point = face_landmarks.landmark[CHIN]\n",
    "                left_face_point = face_landmarks.landmark[LEFT_FACE]\n",
    "                right_face_point = face_landmarks.landmark[RIGHT_FACE]\n",
    "\n",
    "                forehead_y = int(forehead_point.y * frame.shape[0])\n",
    "                chin_y = int(chin_point.y * frame.shape[0])\n",
    "                left_x = int(left_face_point.x * frame.shape[1])\n",
    "                right_x = int(right_face_point.x * frame.shape[1])\n",
    "\n",
    "                hair_height = int((chin_y - forehead_y) * shape_factor[0]) \n",
    "                hair_width = int((right_x - left_x) * shape_factor[1]) \n",
    "\n",
    "                resized_hair_pil = hair.resize((hair_width, hair_height), Image.LANCZOS)\n",
    "                resized_hair = np.array(resized_hair_pil)\n",
    "                \n",
    "                hair_bgr = resized_hair[:, :, :3]\n",
    "                hair_bgr =  cv2.cvtColor(hair_bgr, cv2.COLOR_RGB2BGR) \n",
    "                hair_alpha = resized_hair[:, :, 3] / 255.0\n",
    "                \n",
    "                y_offset = max(0, forehead_y - int(hair_height * position_offset[0]))\n",
    "                x_offset = left_x - int((hair_width - (right_x - left_x)) / position_offset[1])\n",
    "\n",
    "                y1, y2 = y_offset, y_offset + hair_height\n",
    "                x1, x2 = x_offset, x_offset + hair_width\n",
    "                if y2 > frame.shape[0] or x2 > frame.shape[1] or y1 < 0 or x1 < 0:\n",
    "                    continue\n",
    "          \n",
    "                roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "                for c in range(3):\n",
    "                    roi[:, :, c] = roi[:, :, c] * (1 - hair_alpha) + hair_bgr[:, :, c] * hair_alpha\n",
    "\n",
    "                frame[y1:y2, x1:x2] = roi\n",
    "\n",
    "                # Muestra la cámara en color sin el filtro cuando filter_active es False\n",
    "                cv2.imshow(\"Lipstick and Hair Overlay Filter\", frame)\n",
    "\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:  # ESC para salir\n",
    "            break\n",
    "        elif k == ord('1'):  # Tecla \"1\" para activar/desactivar el filtro\n",
    "            filter_active = not filter_active\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
